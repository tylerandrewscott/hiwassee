temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro")
documents<-c(documents,read_file(paste(micro,statements[i],sep="")))
}
# Use regular expressions to clean up some elements of the documents
cleandocs<-c()
for(i in 1:length(documents)){
cleandocs[i]<-gsub("\xfc\xbe\x8d\x96\x90\xbc"," ",documents[i])
cleandocs[i]<-gsub("\r\n"," ",cleandocs[i])
cleandocs[i]<-gsub("\xfc\xbe\x8c\xa3\xa4\xbc"," ",cleandocs[i])
}
# Now, for each of the documents, split them by sentence and search for the
# sentences that contain the word "drought", "disaster", "droughts", "fire",
# "earthquake", "disasters" and take that sentence and the one before and after it as a new document
setwd("/Users/jason/Dropbox/Budgets-APPAM/disaster-budgets/Micro") #output folder
for(i in 1:length(cleandocs)){
temp<-unlist(strsplit(cleandocs[i], "[.]")) #Splits document by sentence
tempname<-unlist(strsplit(statements[i], "[.]"))[1]
tempdisaster<-c()
disaster<-c()
for(j in 1:length(temp)){ # Iterate through the sentences
test<-grep("drought|droughts|water|groundwater|delta|fish|river|rivers",temp[j]) # Does the sentence contain the word drought?
if(length(test)>0){
disaster<-toString(temp[j]) # concatenates sentence with term + sentence before & after term
tempdisaster<-c(tempdisaster,disaster)
}
}
tempdisaster<-toString(tempdisaster)
filename<-paste(tempname,".disaster.txt",sep="")
write(tempdisaster,filename)
}
library(pacman)
# This loads and installs the packages you need at once
pacman::p_load(RTextTools, quanteda,
tm,SnowballC,foreign,plyr,twitteR,
slam,foreign,wordcloud,LiblineaR,e1071, topicmodels,readr)
# Load all the budget statements into R
macro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Macro/"
setwd(macro)
#micro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Micro/"
#setwd(micro)
statements = list.files()
# We want to create the variables: countyname,year,statementtype and read in the documents
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"macro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(macro,statements[i],sep=""))) # Edit here """""
}
# Use regular expressions to clean up some elements of the documents
cleandocs<-c()
for(i in 1:length(documents)){
cleandocs[i]<-gsub("\xfc\xbe\x8d\x96\x90\xbc"," ",documents[i])
cleandocs[i]<-gsub("\r\n"," ",cleandocs[i])
cleandocs[i]<-gsub("\xfc\xbe\x8c\xa3\xa4\xbc"," ",cleandocs[i])
}
setwd("/Users/jason/Dropbox/Budgets-APPAM/disaster-budgets/Macro") #output folder
for(i in 1:length(cleandocs)){
temp<-unlist(strsplit(cleandocs[i], "[.]")) #Splits document by sentence
tempname<-unlist(strsplit(statements[i], "[.]"))[1]
tempdisaster<-c()
disaster<-c()
for(j in 1:length(temp)){ # Iterate through the sentences
test<-grep("drought|droughts|water|groundwater|delta|fish|river|rivers",temp[j]) # Does the sentence contain the word drought?
if(length(test)>0){
disaster<-toString(temp[j]) # concatenates sentence with term + sentence before & after term
tempdisaster<-c(tempdisaster,disaster)
}
}
tempdisaster<-toString(tempdisaster)
filename<-paste(tempname,".disaster.txt",sep="")
write(tempdisaster,filename)
}
ls()
install.packages("lda")
library(lda)
?slda
?hlda
demo(slda)
demo(slda)
library(pacman)
# This loads and installs the packages you need at once
pacman::p_load(RTextTools, quanteda,
tm,SnowballC,foreign,plyr,twitteR,
slam,foreign,wordcloud,LiblineaR,e1071, topicmodels,readr,
lda)
# Load all the budget statements into R
#macro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Macro/"
#setwd(macro)
micro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Micro/"
setwd(micro)
statements = list.files()
cleandocs
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[1])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[2])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[3])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[4])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[5])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[6])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[7])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[8])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[9])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[10])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[11])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[12])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[13])
grep("drought|droughts|water|groundwater|delta|fish|river|rivers",cleandocs[14])
grep("drought|droughts",cleandocs[14])
grep("drought|droughts",cleandocs[13])
grep("drought|droughts",cleandocs[12])
grep("drought|droughts",cleandocs[14])
grep("drought|droughts",cleandocs[14])==0
grep("drought|droughts",cleandocs[14]) > 0
grep("drought|droughts",cleandocs[14]) == 0
numeric(grep("drought|droughts",cleandocs[14]) == 0)
as.numeric(grep("drought|droughts",cleandocs[14]) == 0)
as.numeric(grep("drought|droughts",cleandocs[14]))
as.numeric(grep("drought|droughts",cleandocs[14]))
as.numeric(grep("drought|droughts",cleandocs[11]))
as.numeric(grep("drought|droughts",cleandocs[10]))
as.numeric(grep("drought|droughts",cleandocs[10])) >= 1
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
droughtyear<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(micro,statements[i],sep=""))) # Edit here """""
currentstatement<-read_file(paste(micro,statements[i],sep=""))
# Create a dependent variable for when drought is mentioned
test<-grep("drought|droughts|water|groundwater|delta|fish|river|rivers",
currentstatment)
droughtyear<-c(droughtyear,ifelse(test >= 1, 1,0))
}
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
droughtyear<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(micro,statements[i],sep=""))) # Edit here """""
currentstatement<-read_file(paste(micro,statements[i],sep=""))
# Create a dependent variable for when drought is mentioned
test<-grep("drought|droughts|water|groundwater|delta|fish|river|rivers",
currentstatement)
droughtyear<-c(droughtyear,ifelse(test >= 1, 1,0))
}
droughtyear
test
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
droughtyear<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(micro,statements[i],sep=""))) # Edit here """""
currentstatement<-read_file(paste(micro,statements[i],sep=""))
# Create a dependent variable for when drought is mentioned
test<-grep("drought|droughts",
currentstatement)
droughtyear<-c(droughtyear,ifelse(test >= 1, 1,0))
}
droughtyear
library(pacman)
# This loads and installs the packages you need at once
pacman::p_load(RTextTools, quanteda,
tm,SnowballC,foreign,plyr,twitteR,
slam,foreign,wordcloud,LiblineaR,e1071, topicmodels,readr,
lda)
# Load all the budget statements into R
#macro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Macro/"
#setwd(macro)
micro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Micro/"
setwd(micro)
statements = list.files()
# We want to create the variables: countyname,year,statementtype and read in the documents
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
droughtyear<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(micro,statements[i],sep=""))) # Edit here """""
currentstatement<-read_file(paste(micro,statements[i],sep=""))
# Create a dependent variable for when drought is mentioned
test<-grep("drought|droughts",
currentstatement)
droughtyear<-c(droughtyear,ifelse(test >= 1, 1,0))
}
droughtyear
currentstatement
library(pacman)
# This loads and installs the packages you need at once
pacman::p_load(RTextTools, quanteda,
tm,SnowballC,foreign,plyr,twitteR,
slam,foreign,wordcloud,LiblineaR,e1071, topicmodels,readr,
lda)
# Load all the budget statements into R
#macro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Macro/"
#setwd(macro)
micro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Micro/"
setwd(micro)
statements = list.files()
# We want to create the variables: countyname,year,statementtype and read in the documents
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
droughtyear<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(micro,statements[i],sep=""))) # Edit here """""
}
# Use regular expressions to clean up some elements of the documents
cleandocs<-c()
for(i in 1:length(documents)){
cleandocs[i]<-gsub("\xfc\xbe\x8d\x96\x90\xbc"," ",documents[i])
cleandocs[i]<-gsub("\r\n"," ",cleandocs[i])
cleandocs[i]<-gsub("\xfc\xbe\x8c\xa3\xa4\xbc"," ",cleandocs[i])
}
for(i in 1:length(cleandocs)){
# Create a dependent variable for when drought is mentioned
test<-grep("drought|droughts",
cleandocs[i])
droughtyear<-c(droughtyear,ifelse(test >= 1, 1,0))
}
droughtyear
library(pacman)
# This loads and installs the packages you need at once
pacman::p_load(RTextTools, quanteda,
tm,SnowballC,foreign,plyr,twitteR,
slam,foreign,wordcloud,LiblineaR,e1071, topicmodels,readr,
lda)
# Load all the budget statements into R
#macro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Macro/"
#setwd(macro)
micro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Micro/"
setwd(micro)
statements = list.files()
statements
length(droughtyear)
# We want to create the variables: countyname,year,statementtype and read in the documents
countyname<-c()
year<-c()
statementtype<-c()
documents<-c()
droughtyear<-c()
for(i in 1:length(statements)){
temp<-unlist(strsplit(statements[i], "[.]"))
countyname<-c(countyname,temp[1])
year<-c(year,temp[2])
statementtype<-c(statementtype,"micro") # Edit here when changing the statement type
documents<-c(documents,read_file(paste(micro,statements[i],sep=""))) # Edit here """""
}
length(year)
# Use regular expressions to clean up some elements of the documents
cleandocs<-c()
for(i in 1:length(documents)){
cleandocs[i]<-gsub("\xfc\xbe\x8d\x96\x90\xbc"," ",documents[i])
cleandocs[i]<-gsub("\r\n"," ",cleandocs[i])
cleandocs[i]<-gsub("\xfc\xbe\x8c\xa3\xa4\xbc"," ",cleandocs[i])
}
length(cleandocs)
for(i in 1:length(cleandocs)){
# Create a dependent variable for when drought is mentioned
test<-grep("drought|droughts",
cleandocs[i])
droughtyear<-c(droughtyear,ifelse(test >= 1, 1,0))
}
droughtyear
length(droughtyear)
droughtyear<-c()
for(i in 1:length(cleandocs)){
# Create a dependent variable for when drought is mentioned
droughtyear[i]<- grep("drought|droughts",
cleandocs[i])
}
droughtyear
length(cleandocs)
as.numeric(grep("drought|droughts",cleandocs[10])) >= 1
as.numeric(grep("drought|droughts",cleandocs[10]))
is.numeric(grep("drought|droughts",cleandocs[10]))
is.numeric(grep("drought|droughts",cleandocs[11]))
is.numeric(grep("drought|droughts",cleandocs[115]))
is.numeric(grep("drought|droughts",cleandocs[12])) == 0
is.numeric(grep("drought|droughts",cleandocs[11])) == 0
is.numeric(grep("drought|droughts",cleandocs[10])) == 0
is.numeric(grep("drought|droughts",cleandocs[0])) == 0
grep("drought|droughts",cleandocs[0]))
grep("drought|droughts",cleandocs[1]))
grep("drought|droughts",cleandocs[1])
grep("drought|droughts",cleandocs[0])
grep("drought|droughts",cleandocs[2])
grep("drought|droughts",cleandocs[4])
grep("drought|droughts",cleandocs[5])
length(grep("drought|droughts",cleandocs[5]))
length(grep("drought|droughts",cleandocs[4]))
droughtyear<-c()
for(i in 1:length(cleandocs)){
# Create a dependent variable for when drought is mentioned
droughtyear[i]<- length(grep("drought|droughts",cleandocs[i]))
}
droughtyea
droughtyear
text_cleaner<-function(corpus){
tempcorpus = lapply(corpus,toString)
for(i in 1:length(tempcorpus)){
tempcorpus[[i]]<-iconv(tempcorpus[[i]], "ASCII", "UTF-8", sub="")
}
tempcorpus = lapply(tempcorpus, tolower)
tempcorpus<-Corpus(VectorSource(tempcorpus))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
#Removing all the special charecters and words
tempcorpus <- tm_map(tempcorpus, toSpace, "/")
tempcorpus <- tm_map(tempcorpus, toSpace, "@")
tempcorpus <- tm_map(tempcorpus, toSpace, "\\|")
tempcorpus <- tm_map(tempcorpus, toSpace, "#")
tempcorpus <- tm_map(tempcorpus, toSpace, "http")
tempcorpus <- tm_map(tempcorpus, toSpace, "https")
tempcorpus <- tm_map(tempcorpus, toSpace, ".com")
tempcorpus <- tm_map(tempcorpus, toSpace, "$")
tempcorpus <- tm_map(tempcorpus, removeNumbers)
# Remove english common stopwords
tempcorpus <- tm_map(tempcorpus, removeWords, stopwords("english"))
# Remove punctuation
tempcorpus <- tm_map(tempcorpus, removePunctuation)
# Eliminate extra white spaces
tempcorpus <- tm_map(tempcorpus, stripWhitespace)
# Stem the document
tempcorpus <- tm_map(tempcorpus, PlainTextDocument)
tempcorpus <- tm_map(tempcorpus,  stemDocument, "english")
# Remove uninformative high-frequency words
#tempcorpus <- tm_map(tempcorpus, toSpace, "budget")
tempcorpus <- tm_map(tempcorpus, toSpace, "million")
tempcorpus <- tm_map(tempcorpus, toSpace, "counti")
#tempcorpus <- tm_map(tempcorpus, toSpace, "fund")
tempcorpus <- tm_map(tempcorpus, toSpace, "also")
tempcorpus <- tm_map(tempcorpus, toSpace, "will")
tempcorpus <- tm_map(tempcorpus, toSpace, "board")
tempcorpus <- tm_map(tempcorpus, toSpace, "last")
#tempcorpus <- tm_map(tempcorpus, toSpace, "year")
return(tempcorpus)
}
cleancorpus<-text_cleaner(cleandocs)
# Transform into a document-term matrix
dtm <- DocumentTermMatrix(cleancorpus,
control = list(removePunctuation = TRUE,
stopwords=TRUE,
weighting=weightTf))
# Use only the most frequently occuring words
ten_words<-findFreqTerms(dtm,lowfreq = 10)
dtm10<-DocumentTermMatrix(cleancorpus,
control = list(removePunctuation = TRUE,
stopwords=TRUE,
weighting=weightTf,
dictionary=ten_words
))
dtm10
?slda.em
demo(slda.em)
demo(slda)
poliblog.vocab
poliblog.documents
demo(slda)
data(cora.documents)
corpus <- cora.documents[1:6]
corpus
wc <- word.counts(corpus)
ec
wc
corpus <- cora.documents[1]
corpus
cora.documents[2]
cora.documents[[1]]
cora.documents[[2]]
cora.documents[[3]]
cora.titles
data(cora.vocab)
cora.vocab
length(cora.vocab)
data(cora.titles)
cora.titles[1]
cora.titles[2]
demo(lda)
read.documents(cleancorpus[1])
budgetdocs<-c()
budgetvocab<-c()
## Prep the data for supervised LDA
for(i in 1:length(statements)){
budgetdocs<-c(budgetdocs,read.documents(statements[i]))
budgetvocab<-c(budgetvocab, read.vocab(statements[i]))
}
statements = list.files()
budgetdocs<-c()
budgetvocab<-c()
## Prep the data for supervised LDA
for(i in 1:length(statements)){
budgetdocs<-c(budgetdocs,read.documents(statements[i]))
budgetvocab<-c(budgetvocab, read.vocab(statements[i]))
}
read.documents(statements[i])
read.documents(statements[1])
read.documents(statements[2])
read.documents(statements[3])
read.documents(documents[1])
?read.documents
micro = "~/Dropbox/BudgetsTextAnalysis-PMRC2017/CleanData/Micro/"
setwd(micro)
budgetdocs<-read.documents()
budgetvocab<-read.vocab()
read.documents(dtm10)
lexicalize(documents[1])
testcase<-lexicalize(cleandocs)
testcase
testcase[[1]]
testcase<-lexicalize(documents)
testcase[[1]]
testcase$documents[[1]]
testcase$documents[[2]]
testcase<-lexicalize(cleancorpus)
testcase$documents[[1]]
testcase$documents[[2]]
testcase$documents[[3]]
testcase$documents[[4]]
testcase$documents[[5]]
lexicalize(statement[1])
lexicalize(statements[1])
lexicalize(statements[2])
lexicalize(cleandocs[2])
lexicalize(cleandocs[3])
lexicalize(documents[1])
lexicalize(documents[2])
lexicalize(cleandocs[1])
testcase<-lexicalize(documents)
testcase<-lexicalize(documents[1])
testcase$documents
testcase$vocab
install.packages("tensorflow")
library(tensorflow)
slim = tf$contrib$slim #Poor mans import tensorflow.contrib.slim as slim
tf$reset_default_graph() # Better to start from scratch
# Resizing the images
images = tf$placeholder(tf$float32, shape(NULL, NULL, NULL, 3))
imgs_scaled = tf$image$resize_images(images, shape(224,224))
install.pacakages("magrittr")
library(magrittr)
fc8 = slim$conv2d(imgs_scaled, 64, shape(3,3), scope='vgg_16/conv1/conv1_1') %>%
slim$conv2d(64, shape(3,3), scope='vgg_16/conv1/conv1_2')  %>%
slim$max_pool2d( shape(2, 2), scope='vgg_16/pool1')  %>%
slim$conv2d(128, shape(3,3), scope='vgg_16/conv2/conv2_1')  %>%
slim$conv2d(128, shape(3,3), scope='vgg_16/conv2/conv2_2')  %>%
slim$max_pool2d( shape(2, 2), scope='vgg_16/pool2')  %>%
slim$conv2d(256, shape(3,3), scope='vgg_16/conv3/conv3_1')  %>%
slim$conv2d(256, shape(3,3), scope='vgg_16/conv3/conv3_2')  %>%
slim$conv2d(256, shape(3,3), scope='vgg_16/conv3/conv3_3')  %>%
slim$max_pool2d(shape(2, 2), scope='vgg_16/pool3')  %>%
slim$conv2d(512, shape(3,3), scope='vgg_16/conv4/conv4_1')  %>%
slim$conv2d(512, shape(3,3), scope='vgg_16/conv4/conv4_2')  %>%
slim$conv2d(512, shape(3,3), scope='vgg_16/conv4/conv4_3')  %>%
slim$max_pool2d(shape(2, 2), scope='vgg_16/pool4')  %>%
slim$conv2d(512, shape(3,3), scope='vgg_16/conv5/conv5_1')  %>%
slim$conv2d(512, shape(3,3), scope='vgg_16/conv5/conv5_2')  %>%
slim$conv2d(512, shape(3,3), scope='vgg_16/conv5/conv5_3')  %>%
slim$max_pool2d(shape(2, 2), scope='vgg_16/pool5')  %>%
slim$conv2d(4096, shape(7, 7), padding='VALID', scope='vgg_16/fc6')  %>%
slim$conv2d(4096, shape(1, 1), scope='vgg_16/fc7') %>%
# Setting the activation_fn=NULL does not work, so we get a ReLU
slim$conv2d(1000, shape(1, 1), scope='vgg_16/fc8')  %>%
tf$squeeze(shape(1, 2), name='vgg_16/fc8/squeezed')
tf$train$SummaryWriter('/tmp/dumm/vgg16', tf$get_default_graph())$close()
tensorboard --logdir /tmp/dumm/
exit()
restorer = tf$train$Saver()
sess = tf$Session()
restorer$restore(sess, '/Users/oli/Dropbox/server_sync/tf_slim_models/vgg_16.ckpt')
library(tensorflow)
install_tensorflow()
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
library(tensorflow)
install_tensorflow()
